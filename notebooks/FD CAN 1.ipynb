{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc1175e",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "import hdbscan\n",
    "import itertools\n",
    "import importlib\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from scipy.stats import shapiro, mannwhitneyu, ttest_ind, spearmanr\n",
    "from sklearn.preprocessing import normalize, scale, MinMaxScaler, StandardScaler\n",
    "from scipy.cluster.hierarchy import single, complete, average, ward, dendrogram, linkage, fcluster\n",
    "\n",
    "from tcn import TCN\n",
    "from pandas import read_csv\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm, datasets\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, KFold\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, Activation, GlobalAveragePooling1D, Dense, add, Dropout, concatenate, LSTM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, log_loss, classification_report, silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca0341",
   "metadata": {},
   "source": [
    "## Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac423eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('')\n",
    "\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "data = df.iloc[:, :-2]\n",
    "data = data.iloc[:-1, :]\n",
    "data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0271c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "\n",
    "df_all = df.iloc[:-1, :]  \n",
    "predictor_columns = df_all.iloc[:, :-2]\n",
    "X_full = predictor_columns.values   # Features\n",
    "y_full = df['Fault_Status'].values  # Labels\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_full = scaler.fit_transform(X_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d5de6",
   "metadata": {},
   "source": [
    "## Splitting Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting parameters\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 15\n",
    "BIGGER_SIZE = 25\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          \n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     \n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)      \n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    \n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    \n",
    "plt.rc('legend', fontsize=17)             \n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593bc5b",
   "metadata": {},
   "source": [
    "## TSCV Benchmarking for Fault Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xgb_model():\n",
    "    return xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "def build_rf_model():\n",
    "    return RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "def build_svm_model():\n",
    "    return SVC(kernel='rbf', C=1.0, gamma='auto')\n",
    "\n",
    "def build_lstm_fcn_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # LSTM part\n",
    "    lstm_out = LSTM(64, return_sequences=True)(input_layer)\n",
    "    lstm_out = BatchNormalization()(lstm_out)\n",
    "\n",
    "    # FCN part\n",
    "    conv1 = Conv1D(filters=64, kernel_size=8, padding='same')(input_layer)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    \n",
    "    conv2 = Conv1D(filters=64, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters=64, kernel_size=3, padding='same')(conv2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    \n",
    "    # Combining LSTM and FCN parts\n",
    "    x = concatenate([lstm_out, conv3])\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def inception_block(input_tensor, filters):\n",
    "    conv1 = Conv1D(filters=filters[0], kernel_size=1, activation='relu', padding='same')(input_tensor)\n",
    "    conv3 = Conv1D(filters=filters[1], kernel_size=3, activation='relu', padding='same')(input_tensor)\n",
    "    conv5 = Conv1D(filters=filters[2], kernel_size=5, activation='relu', padding='same')(input_tensor)\n",
    "    pool = MaxPooling1D(pool_size=3, strides=1, padding='same')(input_tensor)\n",
    "    pool_proj = Conv1D(filters=filters[3], kernel_size=1, activation='relu', padding='same')(pool)\n",
    "    return concatenate([conv1, conv3, conv5, pool_proj], axis=-1)\n",
    "\n",
    "def build_inception_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 7, strides=2, padding='same', activation='relu')(input_layer)\n",
    "    x = MaxPooling1D(3, strides=2, padding='same')(x)\n",
    "    x = inception_block(x, [64, 128, 32, 32])\n",
    "    x = inception_block(x, [128, 192, 96, 64])\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)  \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_resnet(input_shape):\n",
    "    n_feature_maps = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # BLOCK 1\n",
    "    conv_x = Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = BatchNormalization()(conv_z)\n",
    "\n",
    "    shortcut_y = Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "    shortcut_y = BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_1 = add([shortcut_y, conv_z])\n",
    "    output_block_1 = Activation('relu')(output_block_1)\n",
    "\n",
    "    # BLOCK 2\n",
    "    conv_x = Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = BatchNormalization()(conv_z)\n",
    "\n",
    "    shortcut_y = Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "    shortcut_y = BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_2 = add([shortcut_y, conv_z])\n",
    "    output_block_2 = Activation('relu')(output_block_2)\n",
    "\n",
    "    # BLOCK 3\n",
    "    conv_x = Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = BatchNormalization()(conv_z)\n",
    "\n",
    "    shortcut_y = BatchNormalization()(output_block_2)\n",
    "\n",
    "    output_block_3 = add([shortcut_y, conv_z])\n",
    "    output_block_3 = Activation('relu')(output_block_3)\n",
    "\n",
    "    gap_layer = GlobalAveragePooling1D()(output_block_3)\n",
    "\n",
    "    output_layer = Dense(1, activation='sigmoid')(gap_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_tcn_model(input_shape):\n",
    "    \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    tcn_layer = TCN(nb_filters=64, kernel_size=3, dilations=[1, 2, 4, 8], padding='causal', return_sequences=True)(input_layer)\n",
    "\n",
    "    pooling_layer = GlobalAveragePooling1D()(tcn_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(pooling_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f34bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_expanding_window_tscv(model_build_fn, model_name, X, y, n_splits, save_path, epochs=100):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    results = []\n",
    "    splits = []\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        unique, counts_train = np.unique(y_train, return_counts=True)\n",
    "        train_distribution = dict(zip(unique, counts_train))\n",
    "\n",
    "        unique, counts_test = np.unique(y_test, return_counts=True)\n",
    "        test_distribution = dict(zip(unique, counts_test))\n",
    "\n",
    "        splits.append((train_index, test_index))\n",
    "\n",
    "        print(f\"Split {i+1}:\")\n",
    "        print(f\"  Training set class distribution: {train_distribution}\")\n",
    "        print(f\"  Testing set class distribution: {test_distribution}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        #if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:  \n",
    "            #print(f\"Skipping split {i+1} as training/testing does not contain both classes.\")\n",
    "            #continue\n",
    "        \n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            X_train, X_test = np.expand_dims(X_train, axis=-1), np.expand_dims(X_test, axis=-1)\n",
    "            model = model_build_fn(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        else:\n",
    "            model = model_build_fn()\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            model.fit(X_train, y_train, epochs=epochs, verbose=0)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        train_time = timeit.default_timer() - start_time\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        results.append({\n",
    "            'split': i+1,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'training_time': train_time,\n",
    "            'time_to_prediction': elapsed\n",
    "        })\n",
    "        K.clear_session()\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "# Running the TSCV\n",
    "save_path = ''\n",
    "models = {\n",
    "    \"XGBoost\": build_xgb_model,\n",
    "    \"RandomForest\": build_rf_model,\n",
    "    \"SupportVectorMachine\": build_svm_model,\n",
    "    \"LSTM_FCN\": build_lstm_fcn_model,\n",
    "    \"InceptionTime\": build_inception_model,\n",
    "    \"ResNet\": build_resnet,\n",
    "    \"TCN\": build_tcn_model\n",
    "}\n",
    "\n",
    "for n_splits in range(8,10):  # From 3 to 6 splits\n",
    "    for model_name, model_fn in models.items():\n",
    "        model_save_path = f\"{save_path}/{model_name}_splits_{n_splits}.csv\"\n",
    "        print(f\"Running {model_name} with {n_splits} splits...\")\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            run_expanding_window_tscv(model_fn, model_name, X, y, n_splits, model_save_path, epochs=100)\n",
    "        else:\n",
    "            run_expanding_window_tscv(model_fn, model_name, X, y, n_splits, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec6e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_sliding_window_tscv(model_build_fn, model_name, X, y, n_splits, test_size, save_path, epochs=100):\n",
    "    total_size = len(X)\n",
    "    train_size = total_size - (n_splits * test_size)  \n",
    "\n",
    "    results = []\n",
    "    splits = []\n",
    "    for i in range(n_splits):\n",
    "        start_train = i * test_size\n",
    "        end_train = start_train + train_size\n",
    "        start_test = end_train\n",
    "        end_test = start_test + test_size\n",
    "\n",
    "        # Ensure we do not go out of bounds\n",
    "        if end_test > total_size:\n",
    "            break\n",
    "\n",
    "        train_index = np.arange(start_train, end_train)\n",
    "        test_index = np.arange(start_test, end_test)\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        unique, counts_train = np.unique(y_train, return_counts=True)\n",
    "        train_distribution = dict(zip(unique, counts_train))\n",
    "        \n",
    "        unique, counts_test = np.unique(y_test, return_counts=True)\n",
    "        test_distribution = dict(zip(unique, counts_test))\n",
    "        \n",
    "        splits.append((train_index, test_index))\n",
    "        \n",
    "        print(f\"Split {i+1}:\")\n",
    "        print(f\"  Training set class distribution: {train_distribution}\")\n",
    "        print(f\"  Testing set class distribution: {test_distribution}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        #if len(np.unique(y_train)) < 2:  # or len(np.unique(y_test)) < 2\n",
    "         #   print(f\"Skipping split {i+1} as training/testing does not contain both classes.\")\n",
    "          #  continue\n",
    "        \n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            X_train, X_test = np.expand_dims(X_train, axis=-1), np.expand_dims(X_test, axis=-1)\n",
    "            model = model_build_fn(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        else:\n",
    "            model = model_build_fn()\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            model.fit(X_train, y_train, epochs=epochs, verbose=0)  \n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        train_time = timeit.default_timer() - start_time\n",
    "        \n",
    "        start_time = timeit.default_timer()\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        #y_pred = (model.predict(X_test))\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        results.append({\n",
    "            'split': i+1,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'training_time': train_time,\n",
    "            'time_to_prediction': elapsed\n",
    "        })\n",
    "\n",
    "        K.clear_session()  \n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "save_path = ''\n",
    "models = {\n",
    "    \"XGBoost\": build_xgb_model,\n",
    "    \"RandomForest\": build_rf_model,\n",
    "    \"SupportVectorMachine\": build_svm_model,\n",
    "    \"LSTM_FCN\": build_lstm_fcn_model,\n",
    "    \"InceptionTime\": build_inception_model,\n",
    "    \"ResNet\": build_resnet,\n",
    "    \"TCN\": build_tcn_model\n",
    "}\n",
    "\n",
    "for n_splits in range(3,10):  # From 3 to 9 splits\n",
    "    test_size = 150  \n",
    "    for model_name, model_fn in models.items():\n",
    "        model_save_path = f\"{save_path}/{model_name}_splits_{n_splits}.csv\"\n",
    "        print(f\"Running {model_name} with {n_splits} splits...\")\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            run_sliding_window_tscv(model_fn, model_name, X, y, n_splits, test_size, model_save_path, epochs=100)\n",
    "        else:\n",
    "            run_sliding_window_tscv(model_fn, model_name, X, y, n_splits, test_size, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d035d0",
   "metadata": {},
   "source": [
    "### Running ROCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e709ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "def convert_to_nested_df(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Convert X_scaled back to a DataFrame to use .iloc\n",
    "    X_scaled = pd.DataFrame(X_scaled)\n",
    "    \n",
    "    nested_df = pd.DataFrame({'series': [pd.Series(X_scaled.iloc[i, :]) for i in range(X_scaled.shape[0])]})\n",
    "    return nested_df\n",
    "\n",
    "def run_expanding_window_tscv_rocket(X, y, n_splits, save_path):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    results = []\n",
    "    \n",
    "    # Convert X to a nested DataFrame appropriate for sktime\n",
    "    X_nested = convert_to_nested_df(X)\n",
    "    \n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X_nested)):\n",
    "        X_train, X_test = X_nested.iloc[train_index], X_nested.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        if len(np.unique(y_train)) < 2:  # or len(np.unique(y_test)) < 2\n",
    "            print(f\"Skipping split {i+1} as training/testing does not contain both classes.\")\n",
    "            continue\n",
    "        \n",
    "        # ROCKET expects nested DataFrame\n",
    "        rocket = Rocket(random_state=42)\n",
    "        start_time = timeit.default_timer()\n",
    "        rocket.fit(X_train)\n",
    "        X_train_transformed = rocket.transform(X_train)\n",
    "        X_test_transformed = rocket.transform(X_test)\n",
    "        \n",
    "        # Classifier training and prediction\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "        classifier.fit(X_train_transformed, y_train)\n",
    "        train_time = timeit.default_timer() - start_time\n",
    "        \n",
    "        start_time = timeit.default_timer()\n",
    "        y_pred = classifier.predict(X_test_transformed)\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        results.append({\n",
    "            'split': i+1,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'training_time': train_time,\n",
    "            'time_to_prediction': elapsed\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "     \n",
    "save_path_base = ''\n",
    "model_name = \"ROCKET\"\n",
    "\n",
    "# Running the TSCV for the ROCKET model\n",
    "for n_splits in range(3, 10):  # From 3 to 6 splits\n",
    "    model_save_path = f\"{save_path_base}/{model_name}_splits_{n_splits}.csv\"\n",
    "    print(f\"Running {model_name} with {n_splits} splits...\")\n",
    "    run_expanding_window_tscv_rocket(X, y, n_splits, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7f909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_results(results, file_path):\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "def sliding_window_tscv(X, y, n_splits=7, test_size=150):\n",
    "\n",
    "    total_size = len(X)\n",
    "    train_size = total_size - (n_splits * test_size)  # Calculate initial training set size\n",
    "    splits = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        start_train = i * test_size\n",
    "        end_train = start_train + train_size\n",
    "        start_test = end_train\n",
    "        end_test = start_test + test_size\n",
    "        \n",
    "        # Ensure we do not go out of bounds\n",
    "        if end_test > total_size:\n",
    "            break\n",
    "        \n",
    "        train_index = np.arange(start_train, end_train)\n",
    "        test_index = np.arange(start_test, end_test)\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        #X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        #y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        unique, counts_train = np.unique(y_train, return_counts=True)\n",
    "        train_distribution = dict(zip(unique, counts_train))\n",
    "        \n",
    "        unique, counts_test = np.unique(y_test, return_counts=True)\n",
    "        test_distribution = dict(zip(unique, counts_test))\n",
    "        \n",
    "        splits.append((train_index, test_index))\n",
    "        \n",
    "        print(f\"Split {i+1}:\")\n",
    "        print(f\"  Training set class distribution: {train_distribution}\")\n",
    "        print(f\"  Testing set class distribution: {test_distribution}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return splits\n",
    "\n",
    "def convert_to_nested_df(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_scaled = pd.DataFrame(X_scaled)\n",
    "    \n",
    "    nested_df = pd.DataFrame({'series': [pd.Series(X_scaled.iloc[i, :]) for i in range(X_scaled.shape[0])]})\n",
    "    return nested_df\n",
    "\n",
    "def run_rocket_with_tscv(X, y, splits, save_path):\n",
    "    X_scaled = convert_to_nested_df(X)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(splits):\n",
    "        print(f\"Processing split {i+1}/{len(splits)}\")\n",
    "        \n",
    "        # Split the data according to the provided indices\n",
    "        X_train, X_test = X_scaled.iloc[train_index], X_scaled.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        if len(np.unique(y_train)) < 2:  # or len(np.unique(y_test)) < 2\n",
    "            print(f\"Skipping split {i+1} as training/testing does not contain both classes.\")\n",
    "            continue\n",
    "\n",
    "        # Initialize and transform data with ROCKET\n",
    "        rocket = Rocket(random_state=42)\n",
    "        \n",
    "        start_time = timeit.default_timer()\n",
    "        rocket.fit(X_train)\n",
    "        X_train_transform = rocket.transform(X_train)\n",
    "        X_test_transform = rocket.transform(X_test)\n",
    "\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "        classifier.fit(X_train_transform, y_train)\n",
    "        train_time = timeit.default_timer() - start_time\n",
    "\n",
    "        # Predict and compute the decision function values\n",
    "        start_time = timeit.default_timer()\n",
    "        y_pred = classifier.predict(X_test_transform)\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        results.append({\n",
    "            'split': i+1,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'training_time': train_time,\n",
    "            'time_to_prediction': elapsed\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "save_path_base = ''  \n",
    "model_name = \"ROCKET\"\n",
    "\n",
    "for n_splits in range(3, 10):  # Adjust the range as needed\n",
    "    model_save_path = f\"{save_path_base}/{model_name}_splits_{n_splits}.csv\"\n",
    "    print(f\"Running {model_name} with {n_splits} splits...\")\n",
    "    splits = sliding_window_tscv(X, y, n_splits, test_size=150)  \n",
    "    results = run_rocket_with_tscv(X, y, splits, model_save_path)  \n",
    "    save_results(results, model_save_path)\n",
    "    print(f\"Results for {n_splits} splits saved to {model_save_path}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
