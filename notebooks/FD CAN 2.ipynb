{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9704c8b5",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d3c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "import hdbscan\n",
    "import itertools\n",
    "import importlib\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from scipy.stats import shapiro, mannwhitneyu, ttest_ind, spearmanr\n",
    "from sklearn.preprocessing import normalize, scale, MinMaxScaler, StandardScaler\n",
    "from scipy.cluster.hierarchy import single, complete, average, ward, dendrogram, linkage, fcluster\n",
    "\n",
    "from tcn import TCN\n",
    "from pandas import read_csv\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm, datasets\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, KFold\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, Activation, GlobalAveragePooling1D, Dense, add, Dropout, concatenate, LSTM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, log_loss, classification_report, silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298e60e",
   "metadata": {},
   "source": [
    "## Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('')\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "data = df.iloc[:, :-2]\n",
    "data = data.iloc[:-1, :]\n",
    "data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "\n",
    "df_all = df.iloc[:-1, :]  \n",
    "predictor_columns = df_all.iloc[:, :-2]\n",
    "X_full = predictor_columns.values   # Features\n",
    "y_full = df['Fault_Status'].values  # Labels\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_full = scaler.fit_transform(X_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25721cf5",
   "metadata": {},
   "source": [
    "## TSCV Benchmarking for Fault Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41323749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting parameters\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 15\n",
    "BIGGER_SIZE = 25\n",
    "\n",
    "plt.rc('font', size=BIGGER_SIZE)          \n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     \n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)      \n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    \n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    \n",
    "plt.rc('legend', fontsize=SMALL_SIZE)             \n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eed7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xgb_model():\n",
    "    return xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "def build_rf_model():\n",
    "    return RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "def build_svm_model():\n",
    "    return SVC(kernel='rbf', C=1.0, gamma='auto')\n",
    "\n",
    "def build_lstm_fcn_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # LSTM part\n",
    "    lstm_out = LSTM(64, return_sequences=True)(input_layer)\n",
    "    lstm_out = BatchNormalization()(lstm_out)\n",
    "\n",
    "    # FCN part\n",
    "    conv1 = Conv1D(filters=64, kernel_size=8, padding='same')(input_layer)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    \n",
    "    conv2 = Conv1D(filters=64, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters=64, kernel_size=3, padding='same')(conv2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    \n",
    "    # Combining LSTM and FCN parts\n",
    "    x = concatenate([lstm_out, conv3])\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def inception_block(input_tensor, filters):\n",
    "    conv1 = Conv1D(filters=filters[0], kernel_size=1, activation='relu', padding='same')(input_tensor)\n",
    "    conv3 = Conv1D(filters=filters[1], kernel_size=3, activation='relu', padding='same')(input_tensor)\n",
    "    conv5 = Conv1D(filters=filters[2], kernel_size=5, activation='relu', padding='same')(input_tensor)\n",
    "    pool = MaxPooling1D(pool_size=3, strides=1, padding='same')(input_tensor)\n",
    "    pool_proj = Conv1D(filters=filters[3], kernel_size=1, activation='relu', padding='same')(pool)\n",
    "    return concatenate([conv1, conv3, conv5, pool_proj], axis=-1)\n",
    "\n",
    "def build_inception_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 7, strides=2, padding='same', activation='relu')(input_layer)\n",
    "    x = MaxPooling1D(3, strides=2, padding='same')(x)\n",
    "    x = inception_block(x, [64, 128, 32, 32])\n",
    "    x = inception_block(x, [128, 192, 96, 64])\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)  \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_resnet(input_shape):\n",
    "    n_feature_maps = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # BLOCK 1\n",
    "    conv_x = Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = BatchNormalization()(conv_z)\n",
    "\n",
    "    shortcut_y = Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "    shortcut_y = BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_1 = add([shortcut_y, conv_z])\n",
    "    output_block_1 = Activation('relu')(output_block_1)\n",
    "\n",
    "    # BLOCK 2\n",
    "    conv_x = Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = BatchNormalization()(conv_z)\n",
    "\n",
    "    shortcut_y = Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "    shortcut_y = BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_2 = add([shortcut_y, conv_z])\n",
    "    output_block_2 = Activation('relu')(output_block_2)\n",
    "\n",
    "    # BLOCK 3\n",
    "    conv_x = Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = BatchNormalization()(conv_z)\n",
    "\n",
    "    shortcut_y = BatchNormalization()(output_block_2)\n",
    "\n",
    "    output_block_3 = add([shortcut_y, conv_z])\n",
    "    output_block_3 = Activation('relu')(output_block_3)\n",
    "\n",
    "    gap_layer = GlobalAveragePooling1D()(output_block_3)\n",
    "\n",
    "    output_layer = Dense(1, activation='sigmoid')(gap_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_tcn_model(input_shape):\n",
    "    \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    tcn_layer = TCN(nb_filters=64, kernel_size=3, dilations=[1, 2, 4, 8], padding='causal', return_sequences=True)(input_layer)\n",
    "\n",
    "    pooling_layer = GlobalAveragePooling1D()(tcn_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(pooling_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipping mechanism\n",
    "if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:  \n",
    "            print(f\"Skipping split {i+1} as it does not contain both classes.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05251200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wf with skipping mechanisim\n",
    "\n",
    "def run_expanding_window_tscv(model_build_fn, model_name, X, y, n_splits, save_path, epochs=100):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    results = []\n",
    "    splits = []\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        unique, counts_train = np.unique(y_train, return_counts=True)\n",
    "        train_distribution = dict(zip(unique, counts_train))\n",
    "        \n",
    "        unique, counts_test = np.unique(y_test, return_counts=True)\n",
    "        test_distribution = dict(zip(unique, counts_test))\n",
    "        \n",
    "        splits.append((train_index, test_index))\n",
    "        \n",
    "        print(f\"Split {i+1}:\")\n",
    "        print(f\"  Training set class distribution: {train_distribution}\")\n",
    "        print(f\"  Testing set class distribution: {test_distribution}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:  \n",
    "            print(f\"Skipping split {i+1} as it does not contain both classes.\")\n",
    "            continue\n",
    "\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            X_train, X_test = np.expand_dims(X_train, axis=-1), np.expand_dims(X_test, axis=-1)\n",
    "            model = model_build_fn(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        else:\n",
    "            model = model_build_fn()\n",
    "        \n",
    "        start_time = timeit.default_timer()\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            model.fit(X_train, y_train, epochs=epochs, verbose=0)  \n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        train_time = timeit.default_timer() - start_time\n",
    "        \n",
    "        start_time = timeit.default_timer()\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_probs = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_probs = model.predict(X_test).squeeze()  # For models that output probabilities directly\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_probs)\n",
    "        \n",
    "        results.append({\n",
    "            'split': i+1,\n",
    "            'roc_auc_score': roc_auc\n",
    "        })\n",
    "        K.clear_session()  \n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "models = {\n",
    "    \"XGBoost\": build_xgb_model,\n",
    "    \"RandomForest\": build_rf_model,\n",
    "    \"SupportVectorMachine\": build_svm_model,\n",
    "    \"LSTM_FCN\": build_lstm_fcn_model,\n",
    "    \"InceptionTime\": build_inception_model,\n",
    "    \"ResNet\": build_resnet,\n",
    "    \"TCN\": build_tcn_model\n",
    "}\n",
    "\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "save_path = ''\n",
    "for n_splits in range(3, 10):  # From 3 to 9 splits\n",
    "    for model_name, model_fn in models.items():\n",
    "        model_save_path = f\"{save_path}/{model_name}_splits_{n_splits}.csv\"\n",
    "        print(f\"Running {model_name} with {n_splits} splits...\")\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            run_expanding_window_tscv(model_fn, model_name, X, y, n_splits, model_save_path, epochs=100)\n",
    "        else:\n",
    "            run_expanding_window_tscv(model_fn, model_name, X, y, n_splits, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c52d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_expanding_window_tscv(model_build_fn, model_name, X, y, n_splits, save_path, epochs=100):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    results = []\n",
    "    splits = []\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        unique, counts_train = np.unique(y_train, return_counts=True)\n",
    "        train_distribution = dict(zip(unique, counts_train))\n",
    "        \n",
    "        unique, counts_test = np.unique(y_test, return_counts=True)\n",
    "        test_distribution = dict(zip(unique, counts_test))\n",
    "        \n",
    "        splits.append((train_index, test_index))\n",
    "        \n",
    "        print(f\"Split {i+1}:\")\n",
    "        print(f\"  Training set class distribution: {train_distribution}\")\n",
    "        print(f\"  Testing set class distribution: {test_distribution}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            X_train, X_test = np.expand_dims(X_train, axis=-1), np.expand_dims(X_test, axis=-1)\n",
    "            model = model_build_fn(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        else:\n",
    "            model = model_build_fn()\n",
    "        \n",
    "        start_time = timeit.default_timer()\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            model.fit(X_train, y_train, epochs=epochs, verbose=0)  \n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "    \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_probs = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_probs = model.predict(X_test).squeeze()  # For models that output probabilities directly\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_probs)\n",
    "        \n",
    "        results.append({\n",
    "            'split': i+1,\n",
    "            'roc_auc_score': roc_auc,\n",
    "            'time_to_prediction': elapsed\n",
    "        })\n",
    "        K.clear_session()  \n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "models = {\n",
    "    \"XGBoost\": build_xgb_model,\n",
    "    \"RandomForest\": build_rf_model,\n",
    "    \"SupportVectorMachine\": build_svm_model,\n",
    "    \"LSTM_FCN\": build_lstm_fcn_model,\n",
    "    \"InceptionTime\": build_inception_model,\n",
    "    \"ResNet\": build_resnet,\n",
    "    \"TCN\": build_tcn_model\n",
    "}\n",
    "\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "save_path = ''\n",
    "for n_splits in range(3, 10):  # From 3 to 9 splits\n",
    "    for model_name, model_fn in models.items():\n",
    "        model_save_path = f\"{save_path}/{model_name}_splits_{n_splits}.csv\"\n",
    "        print(f\"Running {model_name} with {n_splits} splits...\")\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            run_expanding_window_tscv(model_fn, model_name, X, y, n_splits, model_save_path, epochs=100)\n",
    "        else:\n",
    "            run_expanding_window_tscv(model_fn, model_name, X, y, n_splits, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc756c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_sliding_window_tscv(model_build_fn, model_name, X, y, n_splits, test_size, save_path, epochs=100):\n",
    "    total_size = len(X)\n",
    "    train_size = total_size - (n_splits * test_size)  \n",
    "\n",
    "    results = []\n",
    "    splits = []\n",
    "    for i in range(n_splits):\n",
    "        start_train = i * test_size\n",
    "        end_train = start_train + train_size\n",
    "        start_test = end_train\n",
    "        end_test = start_test + test_size\n",
    "\n",
    "        # Ensure we do not go out of bounds\n",
    "        if end_test > total_size:\n",
    "            break\n",
    "\n",
    "        train_index = np.arange(start_train, end_train)\n",
    "        test_index = np.arange(start_test, end_test)\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        unique, counts_train = np.unique(y_train, return_counts=True)\n",
    "        train_distribution = dict(zip(unique, counts_train))\n",
    "        \n",
    "        unique, counts_test = np.unique(y_test, return_counts=True)\n",
    "        test_distribution = dict(zip(unique, counts_test))\n",
    "        \n",
    "        splits.append((train_index, test_index))\n",
    "        \n",
    "        print(f\"Split {i+1}:\")\n",
    "        print(f\"  Training set class distribution: {train_distribution}\")\n",
    "        print(f\"  Testing set class distribution: {test_distribution}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:  \n",
    "            print(f\"Warning: Only one class present in y_test for Split {i+1}, skipping ROC AUC calculation.\")\n",
    "            roc_auc = 0  # Assign NaN or a specific value if you prefer\n",
    "            continue\n",
    "        else:\n",
    "            if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "                X_train, X_test = np.expand_dims(X_train, axis=-1), np.expand_dims(X_test, axis=-1)\n",
    "                model = model_build_fn(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "            else:\n",
    "                model = model_build_fn()\n",
    "            \n",
    "            #start_time = timeit.default_timer()\n",
    "            if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "                model.fit(X_train, y_train, epochs=epochs, verbose=0)  \n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            #elapsed = timeit.default_timer() - start_time\n",
    "            \n",
    "            y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "            #y_pred = (model.predict(X_test))\n",
    "\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_probs = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_probs = model.predict(X_test).squeeze()  # For models that output probabilities directly\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_probs)\n",
    "        \n",
    "        results.append({\n",
    "            'split': i+1,\n",
    "            'roc_auc_score': roc_auc\n",
    "        })\n",
    "        K.clear_session()  \n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "save_path = ''\n",
    "models = {\n",
    "    \"XGBoost\": build_xgb_model,\n",
    "    \"RandomForest\": build_rf_model,\n",
    "    \"SupportVectorMachine\": build_svm_model,\n",
    "    \"LSTM_FCN\": build_lstm_fcn_model,\n",
    "    \"InceptionTime\": build_inception_model,\n",
    "    \"ResNet\": build_resnet,\n",
    "    \"TCN\": build_tcn_model\n",
    "}\n",
    "\n",
    "for n_splits in range(3,10):  # From 3 to 9 splits\n",
    "    test_size = 150  \n",
    "    for model_name, model_fn in models.items():\n",
    "        model_save_path = f\"{save_path}/{model_name}_splits_{n_splits}.csv\"\n",
    "        print(f\"Running {model_name} with {n_splits} splits...\")\n",
    "        if model_name in [\"LSTM_FCN\", \"InceptionTime\", \"ResNet\", \"TCN\"]:\n",
    "            run_sliding_window_tscv(model_fn, model_name, X, y, n_splits, test_size, model_save_path, epochs=100)\n",
    "        else:\n",
    "            run_sliding_window_tscv(model_fn, model_name, X, y, n_splits, test_size,model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66a711",
   "metadata": {},
   "source": [
    "### ROCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2d8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wf\n",
    "def convert_to_nested_df(X):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled)\n",
    "    nested_df = pd.DataFrame({'series': [pd.Series(X_scaled.iloc[i, :]) for i in range(X_scaled.shape[0])]})\n",
    "    return nested_df\n",
    "\n",
    "def walk_forward_tscv(X, y, n_splits=7):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    results = []\n",
    "\n",
    "    # Assuming X is already a DataFrame at this point; if not, it should be converted before this function\n",
    "    X_nested = convert_to_nested_df(X)\n",
    "    \n",
    "    rocket = Rocket(random_state=42)\n",
    "    classifier = LogisticRegression()\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X_nested)):\n",
    "        X_train, X_test = X_nested.iloc[train_index], X_nested.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:  # \n",
    "            print(f\"Skipping split {i+1} as training/testing does not contain both classes.\")\n",
    "            continue\n",
    "\n",
    "        rocket.fit(X_train)\n",
    "        X_train_transform = rocket.transform(X_train)\n",
    "        X_test_transform = rocket.transform(X_test)\n",
    "\n",
    "        classifier.fit(X_train_transform, y_train)\n",
    "        y_pred = classifier.predict(X_test_transform)\n",
    "        #y_pred_proba = classifier.predict_proba(X_test_transform)[:, 1]\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        #results.append(roc_auc)\n",
    "        results.append({'split': i + 1, 'roc_auc_score': roc_auc})\n",
    "        print(f\"Split ROC-AUC: {roc_auc}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results, file_path):\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "save_path_base = ''\n",
    "model_name = \"ROCKET\"\n",
    "\n",
    "# Running the TSCV for the ROCKET model\n",
    "for n_splits in range(3, 10):  # From 3 to 6 splits\n",
    "    model_save_path = f\"{save_path_base}/{model_name}_splits_{n_splits}.csv\"\n",
    "    print(f\"Running {model_name} with {n_splits} splits...\")\n",
    "    results = walk_forward_tscv(X, y, n_splits)\n",
    "    save_results(results, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43953439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sw \n",
    "\n",
    "def save_results(results, file_path):\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "def convert_to_nested_df(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled)\n",
    "    nested_df = pd.DataFrame({'series': [pd.Series(X_scaled.iloc[i, :]) for i in range(X_scaled.shape[0])]})\n",
    "    return nested_df\n",
    "\n",
    "def sliding_window_tscv(X, y, n_splits=7, test_size=150):\n",
    "    total_size = len(X)\n",
    "    train_size = total_size - (n_splits * test_size)\n",
    "    splits = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        start_train = i * test_size\n",
    "        end_train = start_train + train_size\n",
    "        start_test = end_train\n",
    "        end_test = start_test + test_size\n",
    "        \n",
    "        if end_test > total_size:\n",
    "            break\n",
    "        \n",
    "        train_index = np.arange(start_train, end_train)\n",
    "        test_index = np.arange(start_test, end_test)\n",
    "        splits.append((train_index, test_index))\n",
    "        \n",
    "    return splits\n",
    "\n",
    "def run_rocket_with_tscv(X, y, splits, save_path):\n",
    "    X_scaled = convert_to_nested_df(X)\n",
    "    results = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(splits):\n",
    "        print(f\"Processing split {i+1}/{len(splits)}\")\n",
    "        \n",
    "        X_train, X_test = X_scaled.iloc[train_index], X_scaled.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:  \n",
    "            print(f\"Skipping split {i+1} as it does not contain both classes.\")\n",
    "            continue  \n",
    "\n",
    "        rocket = Rocket(random_state=42)\n",
    "        rocket.fit(X_train)\n",
    "        X_train_transform = rocket.transform(X_train)\n",
    "        X_test_transform = rocket.transform(X_test)\n",
    "\n",
    "        classifier = LogisticRegression()\n",
    "        classifier.fit(X_train_transform, y_train)\n",
    "        y_pred_proba = classifier.predict_proba(X_test_transform)[:, 1]\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        results.append({'split': i + 1, 'roc_auc_score': roc_auc})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "save_path_base = ''\n",
    "model_name = \"ROCKET\"\n",
    "\n",
    "for n_splits in range(3, 10):  # From 3 to 9 splits\n",
    "    model_save_path = f\"{save_path_base}/{model_name}_splits_{n_splits}.csv\"\n",
    "    print(f\"Running {model_name} with {n_splits} splits...\")\n",
    "    splits = sliding_window_tscv(X, y, n_splits, test_size=150)\n",
    "    results = run_rocket_with_tscv(X, y, splits, model_save_path)\n",
    "    save_results(results, model_save_path)\n",
    "    print(f\"Results for {n_splits} splits saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
