{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42959684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "import hdbscan\n",
    "import itertools\n",
    "import importlib\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from scipy.stats import shapiro, mannwhitneyu, ttest_ind, spearmanr\n",
    "from sklearn.preprocessing import normalize, scale, MinMaxScaler, StandardScaler\n",
    "from scipy.cluster.hierarchy import single, complete, average, ward, dendrogram, linkage, fcluster\n",
    "\n",
    "from tcn import TCN\n",
    "from pandas import read_csv\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm, datasets\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, KFold\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, Activation, GlobalAveragePooling1D, Dense, add, Dropout, concatenate, LSTM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, log_loss, classification_report, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e4203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate statistics\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the file structure and model names\n",
    "base_path = ''\n",
    "metrics = {\n",
    "    'f1': 'f1_score',\n",
    "    'roc': 'roc_auc_score',\n",
    "    'mcc': 'mcc'\n",
    "}\n",
    "methods = ['wf', 'bw', 'sw']\n",
    "models = ['XGBoost', 'RandomForest', 'SupportVectorMachine', 'ROCKET', 'InceptionTime', 'LSTM_FCN', 'ResNet', 'TCN']\n",
    "k_folds = [3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results_list = []\n",
    "\n",
    "# Loop through each metric, method, model, and k-fold to read CSV files and calculate statistics\n",
    "for metric, column_name in metrics.items():\n",
    "    for method in methods:\n",
    "        for model in models:\n",
    "            for k in k_folds:\n",
    "                file_name = f'{model}_splits_{k}.csv'\n",
    "                file_path = os.path.join(base_path, metric, method, file_name)\n",
    "\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        # Read the CSV file\n",
    "                        data = pd.read_csv(file_path)\n",
    "                        \n",
    "                        # Check if the data is not empty\n",
    "                        if not data.empty:\n",
    "                            # Calculate the statistics for the specified column\n",
    "                            ave = data[column_name].mean()\n",
    "                            std = data[column_name].std(ddof=0)\n",
    "                            med = data[column_name].median()\n",
    "\n",
    "                            # Append the results to the list\n",
    "                            results_list.append({\n",
    "                                'Model': model,\n",
    "                                'K-fold': k,\n",
    "                                'TSCV Method': method,\n",
    "                                'Metric': metric.upper(),\n",
    "                                'AVE': ave,\n",
    "                                'STD': std,\n",
    "                                'MED': med\n",
    "                            })\n",
    "                    except pd.errors.EmptyDataError:\n",
    "                        print(f\"File {file_path} is empty and will be skipped.\")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=['Model', 'K-fold', 'TSCV Method', 'Metric', 'AVE', 'STD', 'MED'])\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "results_df.to_csv()\n",
    "print(\"Results have been saved to 'benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93bb6e",
   "metadata": {},
   "source": [
    "# Rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e0a50",
   "metadata": {},
   "source": [
    "## Classifier ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4036f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rank_models_mcc(df, k_folds):\n",
    "    results = []\n",
    "    \n",
    "    for k in k_folds:\n",
    "        k_df = df[df['K-fold'] == k]\n",
    "        \n",
    "        for split in k_df['split'].unique():\n",
    "            split_df = k_df[k_df['split'] == split].sort_values(by='mcc', ascending=False).reset_index(drop=True)\n",
    "            split_df['rank'] = split_df.index + 1\n",
    "            split_df['points'] = split_df['rank']  # Best model gets the lowest points\n",
    "            results.append(split_df)\n",
    "    \n",
    "    result_df = pd.concat(results)\n",
    "    \n",
    "    # Calculate the average points for each model across all splits for each k-fold\n",
    "    avg_points_df = result_df.groupby(['Model', 'K-fold'])['points'].mean().reset_index()\n",
    "    avg_points_df = avg_points_df.sort_values(by='points', ascending=True)\n",
    "    \n",
    "    return avg_points_df\n",
    "\n",
    "def rank_models_roc(df, k_folds):\n",
    "    results = []\n",
    "    \n",
    "    for k in k_folds:\n",
    "        k_df = df[df['K-fold'] == k]\n",
    "        \n",
    "        for split in k_df['split'].unique():\n",
    "            split_df = k_df[k_df['split'] == split].sort_values(by='roc_auc_score', ascending=False).reset_index(drop=True)\n",
    "            split_df['rank'] = split_df.index + 1\n",
    "            split_df['points'] = split_df['rank']  # Best model gets the lowest points\n",
    "            results.append(split_df)\n",
    "    \n",
    "    result_df = pd.concat(results)\n",
    "    \n",
    "    # Calculate the average points for each model across all splits for each k-fold\n",
    "    avg_points_df = result_df.groupby(['Model', 'K-fold'])['points'].mean().reset_index()\n",
    "    avg_points_df = avg_points_df.sort_values(by='points', ascending=True)\n",
    "    \n",
    "    return avg_points_df\n",
    "\n",
    "def rank_models_f1(df, k_folds):\n",
    "    results = []\n",
    "    \n",
    "    for k in k_folds:\n",
    "        k_df = df[df['K-fold'] == k]\n",
    "        \n",
    "        for split in k_df['split'].unique():\n",
    "            split_df = k_df[k_df['split'] == split].sort_values(by='f1_score', ascending=False).reset_index(drop=True)\n",
    "            split_df['rank'] = split_df.index + 1\n",
    "            split_df['points'] = split_df['rank']  # Best model gets the lowest points\n",
    "            results.append(split_df)\n",
    "    \n",
    "    result_df = pd.concat(results)\n",
    "    \n",
    "    # Calculate the average points for each model across all splits for each k-fold\n",
    "    avg_points_df = result_df.groupby(['Model', 'K-fold'])['points'].mean().reset_index()\n",
    "    avg_points_df = avg_points_df.sort_values(by='points', ascending=True)\n",
    "    \n",
    "    return avg_points_df\n",
    "\n",
    "k_folds = [3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "path_mcc = r'/mcc/'\n",
    "wf_mcc_df = pd.read_csv(path_mcc + 'SortedMCC_tscv_wf.csv')\n",
    "bw_mcc_df = pd.read_csv(path_mcc + 'SortedMCC_tscv_bw.csv')\n",
    "sw_mcc_df = pd.read_csv(path_mcc + 'SortedMCC_tscv_sw.csv')\n",
    "\n",
    "wf_mcc_avg_points = rank_models_mcc(wf_mcc_df, k_folds)\n",
    "bw_mcc_avg_points = rank_models_mcc(bw_mcc_df, k_folds)\n",
    "sw_mcc_avg_points = rank_models_mcc(sw_mcc_df, k_folds)\n",
    "\n",
    "# Save the results to CSV files\n",
    "wf_mcc_avg_points.to_csv(path_mcc + 'wf_avg_points.csv', index=False)\n",
    "bw_mcc_avg_points.to_csv(path_mcc + 'bw_avg_points.csv', index=False)\n",
    "sw_mcc_avg_points.to_csv(path_mcc + 'sw_avg_points.csv', index=False)\n",
    "\n",
    "path_roc = r'/roc/'\n",
    "wf_roc_df = pd.read_csv(path_roc + 'SortedROC_tscv_wf.csv')\n",
    "bw_roc_df = pd.read_csv(path_roc + 'SortedROC_tscv_bw.csv')\n",
    "sw_roc_df = pd.read_csv(path_roc + 'SortedROC_tscv_sw.csv')\n",
    "\n",
    "wf_roc_avg_points = rank_models_roc(wf_roc_df, k_folds)\n",
    "bw_roc_avg_points = rank_models_roc(bw_roc_df, k_folds)\n",
    "sw_roc_avg_points = rank_models_roc(sw_roc_df, k_folds)\n",
    "\n",
    "# Save the results to CSV files\n",
    "wf_roc_avg_points.to_csv(path_roc + 'wf_avg_points.csv', index=False)\n",
    "bw_roc_avg_points.to_csv(path_roc + 'bw_avg_points.csv', index=False)\n",
    "sw_roc_avg_points.to_csv(path_roc + 'sw_avg_points.csv', index=False)\n",
    "\n",
    "path_f1 = r'/f1/'\n",
    "wf_f1_df = pd.read_csv(path_f1 + 'SortedF1_tscv_wf.csv')\n",
    "bw_f1_df = pd.read_csv(path_f1 + 'SortedF1_tscv_bw.csv')\n",
    "sw_f1_df = pd.read_csv(path_f1 + 'SortedF1_tscv_sw.csv')\n",
    "\n",
    "wf_f1_avg_points = rank_models_f1(wf_f1_df, k_folds)\n",
    "bw_f1_avg_points = rank_models_f1(bw_f1_df, k_folds)\n",
    "sw_f1_avg_points = rank_models_f1(sw_f1_df, k_folds)\n",
    "\n",
    "# Display the results\n",
    "print(\"Walk-Forward (WF) Average Points\")\n",
    "print(wf_f1_avg_points)\n",
    "print(\"\\nBlocking Window (BW) Average Points\")\n",
    "print(bw_f1_avg_points)\n",
    "print(\"\\nSliding Window (SW) Average Points\")\n",
    "print(sw_f1_avg_points)\n",
    "\n",
    "# Save the results to CSV files\n",
    "wf_f1_avg_points.to_csv(path_f1 + 'wf_avg_points.csv', index=False)\n",
    "bw_f1_avg_points.to_csv(path_f1 + 'bw_avg_points.csv', index=False)\n",
    "sw_f1_avg_points.to_csv(path_f1 + 'sw_avg_points.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ee7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path1 = r'/f1/'\n",
    "path2 = r'/roc/'\n",
    "path3 = r'/mcc/'\n",
    "\n",
    "# Load the ranking CSV files\n",
    "wf_f1_df = pd.read_csv(path1+'wf_avg_points.csv')\n",
    "wf_roc_df = pd.read_csv(path2+'wf_avg_points.csv')\n",
    "wf_mcc_df = pd.read_csv(path3+'wf_avg_points.csv')\n",
    "\n",
    "bw_f1_df = pd.read_csv(path1+'bw_avg_points.csv')\n",
    "bw_roc_df = pd.read_csv(path2+'bw_avg_points.csv')\n",
    "bw_mcc_df = pd.read_csv(path3+'bw_avg_points.csv')\n",
    "\n",
    "sw_f1_df = pd.read_csv(path1+'sw_avg_points.csv')\n",
    "sw_roc_df = pd.read_csv(path2+'sw_avg_points.csv')\n",
    "sw_mcc_df = pd.read_csv(path3+'sw_avg_points.csv')\n",
    "\n",
    "# Combine DataFrames for each TSCV method\n",
    "wf_df = pd.concat([wf_f1_df.assign(Metric='F1'), wf_roc_df.assign(Metric='ROC'), wf_mcc_df.assign(Metric='MCC')])\n",
    "bw_df = pd.concat([bw_f1_df.assign(Metric='F1'), bw_roc_df.assign(Metric='ROC'), bw_mcc_df.assign(Metric='MCC')])\n",
    "sw_df = pd.concat([sw_f1_df.assign(Metric='F1'), sw_roc_df.assign(Metric='ROC'), sw_mcc_df.assign(Metric='MCC')])\n",
    "\n",
    "# Plotting Function\n",
    "def plot_ranking(df, method):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Sort data by points in descending order within each k-fold\n",
    "    df = df.sort_values(by=['K-fold', 'points'], ascending=[True, False])\n",
    "    \n",
    "    sns.barplot(x='K-fold', y='points', ci=None, hue='Model', data=df, dodge=True)\n",
    "    plt.title(f'{method} Method - Model Ranking by Average Points')\n",
    "    plt.xlabel('K-fold')\n",
    "    plt.ylabel('Average Points')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "# Plot rankings for each TSCV method\n",
    "plot_ranking(wf_df, 'Walk-Forward (WF)')\n",
    "plot_ranking(bw_df, 'Blocking Window (BW)')\n",
    "plot_ranking(sw_df, 'Sliding Window (SW)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcf854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "# Combine DataFrames for each TSCV method\n",
    "wf_df = pd.concat([wf_f1_avg_points.assign(Metric='F1'), wf_roc_avg_points.assign(Metric='ROC'), wf_mcc_avg_points.assign(Metric='MCC')])\n",
    "bw_df = pd.concat([bw_f1_avg_points.assign(Metric='F1'), bw_roc_avg_points.assign(Metric='ROC'), bw_mcc_avg_points.assign(Metric='MCC')])\n",
    "sw_df = pd.concat([sw_f1_avg_points.assign(Metric='F1'), sw_roc_avg_points.assign(Metric='ROC'), sw_mcc_avg_points.assign(Metric='MCC')])\n",
    "\n",
    "# Combine all DataFrames for easier plotting\n",
    "combined_df = pd.concat([wf_df.assign(Method='WF'), bw_df.assign(Method='BW'), sw_df.assign(Method='SW')])\n",
    "\n",
    "# Define models and their respective markers\n",
    "models = combined_df['Model'].unique()\n",
    "colors = ['#d7191c', '#fdae61', '#a6d96a', '#1a9641', '#2c7bb6', '#abd9e9', '#fdae61', '#f46d43']\n",
    "markers = ['o', 's', 'd', '^', 'v', '<', '>', 'p']\n",
    "\n",
    "# Define k-folds\n",
    "k_folds = [3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# xytext offsets for annotations\n",
    "xytext_offsets = {\n",
    "    (0, 2, 4): (-10, -45),\n",
    "}\n",
    "\n",
    "# Function to get xytext offset for each annotation\n",
    "def get_xytext_offset(key):\n",
    "    return xytext_offsets.get(key, (0, -45))\n",
    "\n",
    "def plot_ranking(df, k_folds, models, metrics, colors, markers):\n",
    "    fig, axs = plt.subplots(1, len(k_folds), figsize=(25, 10), sharey=True)\n",
    "    \n",
    "    for idx, k in enumerate(k_folds):\n",
    "        k_df = df[df['K-fold'] == k]\n",
    "        ranking_df = []\n",
    "        \n",
    "        for method in ['WF', 'BW', 'SW']:\n",
    "            for metric in metrics:\n",
    "                metric_df = k_df[(k_df['Method'] == method) & (k_df['Metric'] == metric)].sort_values(by='points', ascending=True)\n",
    "                metric_df['Rank'] = range(1, len(metric_df) + 1)\n",
    "                metric_df['Method_Metric'] = f'{method}-{metric}'\n",
    "                ranking_df.append(metric_df)\n",
    "        \n",
    "        ranking_df = pd.concat(ranking_df)\n",
    "        ranking_df['Method_Metric'] = pd.Categorical(ranking_df['Method_Metric'], categories=[\n",
    "            f'WF-{metric}' for metric in metrics] + [f'BW-{metric}' for metric in metrics] + [f'SW-{metric}' for metric in metrics], ordered=True)\n",
    "        \n",
    "        for model in models:\n",
    "            model_df = ranking_df[ranking_df['Model'] == model]\n",
    "            axs[idx].semilogx(model_df['points'], model_df['Method_Metric'], label=model, marker=markers[np.where(models == model)[0][0]], color=colors[np.where(models == model)[0][0]], linestyle='', markersize=10)\n",
    "            # Add annotations with actual values\n",
    "            for j, row in model_df.iterrows():\n",
    "                xytext = get_xytext_offset((idx, np.where(models == model)[0][0], j))\n",
    "                axs[idx].annotate(f'{row[\"points\"]:.2f}', (row['points'], row['Method_Metric']), textcoords=\"offset points\", xytext=xytext, ha='center', fontsize=9, color=colors[np.where(models == model)[0][0]])\n",
    "                # Temp labels for identification, spaced out vertically\n",
    "                #axs[i].annotate(f'{i}-{j}-{k}', (data[j, k], k), rotation=0, textcoords=\"offset points\", xytext=(0, 25 + j * 15), ha='center', fontsize=12, color='blue', bbox=dict(facecolor='yellow', alpha=0.7))\n",
    "        \n",
    "        axs[idx].set_xscale('log')\n",
    "        axs[idx].set_xlim(2.2, 7)  # Adjusted to fit the data better\n",
    "        axs[idx].set_xticks([3, 3.5, 4, 4.5, 5, 5.5, 6])\n",
    "        axs[idx].set_xticklabels(['3', '3.5', '4', '4.5', '5', '5.5', '6'], fontsize=12)\n",
    "        axs[idx].set_title(f'K-fold = {k}', fontsize=20)\n",
    "        axs[idx].set_xlabel('Average Points (Log Scale)', fontsize=15)\n",
    "        axs[idx].invert_yaxis()  # Highest rank (1) at the top\n",
    "        \n",
    "    axs[0].set_ylabel('Method-Metric', fontsize=15)\n",
    "    handles = [Line2D([0], [0], color=color, marker=marker, linestyle='', markersize=14) for color, marker in zip(colors, markers)]\n",
    "    fig.legend(handles, models, fontsize=17, loc='upper center', ncol=8, bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define metrics\n",
    "metrics = ['F1', 'ROC', 'MCC']\n",
    "\n",
    "# Plot rankings for each k-fold\n",
    "plot_ranking(combined_df, k_folds, models, metrics, colors, markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b8246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Changing the plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Combine DataFrames for each TSCV method\n",
    "wf_df = pd.concat([wf_f1_avg_points.assign(Metric='F1'), wf_roc_avg_points.assign(Metric='ROC'), wf_mcc_avg_points.assign(Metric='MCC')])\n",
    "bw_df = pd.concat([bw_f1_avg_points.assign(Metric='F1'), bw_roc_avg_points.assign(Metric='ROC'), bw_mcc_avg_points.assign(Metric='MCC')])\n",
    "sw_df = pd.concat([sw_f1_avg_points.assign(Metric='F1'), sw_roc_avg_points.assign(Metric='ROC'), sw_mcc_avg_points.assign(Metric='MCC')])\n",
    "\n",
    "# Combine all DataFrames for easier plotting\n",
    "combined_df = pd.concat([wf_df.assign(Method='WF'), bw_df.assign(Method='BW'), sw_df.assign(Method='SW')])\n",
    "\n",
    "# Define models and their respective markers\n",
    "models = combined_df['Model'].unique()\n",
    "colors = ['#d7191c', '#fdae61', '#a6d96a', '#1a9641', '#2c7bb6', '#abd9e9', '#fdae61', '#f46d43']\n",
    "markers = ['o', 's', 'd', '^', 'x', '*', '_', '+']\n",
    "\n",
    "# Define k-folds\n",
    "k_folds = [3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "def plot_ranking(df, method, models, metrics, colors, markers):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(20, 30), sharex=True)\n",
    "    \n",
    "    for metric_idx, metric in enumerate(metrics):\n",
    "        metric_df = df[(df['Method'] == method) & (df['Metric'] == metric)]\n",
    "        for model in models:\n",
    "            model_df = metric_df[metric_df['Model'] == model]\n",
    "            axs[metric_idx].semilogx(model_df['points'], model_df['K-fold'], label=model, marker=markers[np.where(models == model)[0][0]], color=colors[np.where(models == model)[0][0]], linestyle='', markersize=12, alpha=0.7)\n",
    "            # Add annotations with actual values\n",
    "            for j, row in model_df.iterrows():\n",
    "                axs[metric_idx].annotate(f'{row[\"points\"]:.1f}', (row['points'], row['K-fold']), textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=12, color=colors[np.where(models == model)[0][0]])\n",
    "        \n",
    "        axs[metric_idx].set_xscale('log')\n",
    "        axs[metric_idx].set_xlim(1.1, 7)  \n",
    "        axs[metric_idx].set_xticks([3, 3.5, 4, 4.5, 5, 5.5, 6])\n",
    "        axs[metric_idx].set_xticklabels(['3', '3.5', '4', '4.5', '5', '5.5', '6'], fontsize=17)\n",
    "        axs[metric_idx].set_yticks(k_folds)\n",
    "        axs[metric_idx].set_yticklabels([f'K-fold = {k}' for k in k_folds], fontsize=17)\n",
    "        axs[metric_idx].invert_yaxis()  \n",
    "        #axs[metric_idx].set_ylabel(f'{metric}', fontsize=17)\n",
    "        axs[metric_idx].set_title(f'{metric} Score Ranking', fontsize=20)\n",
    "    \n",
    "    axs[2].set_xlabel('Average Points (Log Scale)', fontsize=15)\n",
    "    handles = [Line2D([0], [0], color=color, marker=marker, linestyle='', markersize=14) for color, marker in zip(colors, markers)]\n",
    "    fig.legend(handles, models, fontsize=17, loc='upper center', ncol=8, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "metrics = ['F1', 'ROC', 'MCC']\n",
    "\n",
    "# Plot rankings for each method\n",
    "for method in ['WF', 'BW', 'SW']:\n",
    "    plot_ranking(combined_df, method, models, metrics, colors, markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9a78c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "wf_df = pd.concat([wf_f1_avg_points.assign(Metric='F1'), wf_roc_avg_points.assign(Metric='ROC'), wf_mcc_avg_points.assign(Metric='MCC')])\n",
    "bw_df = pd.concat([bw_f1_avg_points.assign(Metric='F1'), bw_roc_avg_points.assign(Metric='ROC'), bw_mcc_avg_points.assign(Metric='MCC')])\n",
    "sw_df = pd.concat([sw_f1_avg_points.assign(Metric='F1'), sw_roc_avg_points.assign(Metric='ROC'), sw_mcc_avg_points.assign(Metric='MCC')])\n",
    "\n",
    "# Combine all DataFrames for easier plotting\n",
    "combined_df = pd.concat([wf_df.assign(Method='WF'), bw_df.assign(Method='BW'), sw_df.assign(Method='SW')])\n",
    "\n",
    "# Define models and their respective markers\n",
    "models = combined_df['Model'].unique()\n",
    "colors = ['#d7191c', '#fdae61', '#a6d96a', '#1a9641', '#2c7bb6', '#abd9e9', '#fdae61', '#f46d43']\n",
    "markers = ['o', 's', 'd', '^', 'x', '*', '_', '+']\n",
    "\n",
    "# Define k-folds\n",
    "k_folds = [3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "def plot_ranking(df, method, models, metrics, colors, markers):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(20, 30), sharex=True)\n",
    "    \n",
    "    for metric_idx, metric in enumerate(metrics):\n",
    "        metric_df = df[(df['Method'] == method) & (df['Metric'] == metric)]\n",
    "        for model in models:\n",
    "            model_df = metric_df[metric_df['Model'] == model]\n",
    "            axs[metric_idx].semilogx(model_df['points'], model_df['K-fold'], label=model, marker=markers[np.where(models == model)[0][0]], color=colors[np.where(models == model)[0][0]], linestyle='', markersize=20, alpha=0.7)\n",
    "            # Add annotations with actual values\n",
    "            for j, row in model_df.iterrows():\n",
    "                axs[metric_idx].annotate(f'{row[\"points\"]:.1f}', (row['points'], row['K-fold']), textcoords=\"offset points\", xytext=(0, 12), ha='center', fontsize=14, color=colors[np.where(models == model)[0][0]])\n",
    "        \n",
    "        axs[metric_idx].set_xscale('log', base=2)\n",
    "        axs[metric_idx].set_xlim(0.9, 7)  \n",
    "        axs[metric_idx].set_xticks([1, 1.5, 2, 2.5, 3, 4, 5, 6, 8])\n",
    "        axs[metric_idx].set_xticklabels(['1', '1.5', '2', '2.5', '3', '4', '5', '6', '7'], fontsize=17)\n",
    "        axs[metric_idx].set_yticks(k_folds)\n",
    "        axs[metric_idx].set_yticklabels([f'K-fold = {k}' for k in k_folds], fontsize=35)\n",
    "        axs[metric_idx].invert_yaxis()  \n",
    "        axs[metric_idx].set_title(f'{metric} Score Ranking', fontsize=35)\n",
    "    \n",
    "    axs[2].set_xlabel('Average Rank (Base 2)', fontsize=30)\n",
    "    handles = [Line2D([0], [0], color=color, marker=marker, linestyle='', markersize=25) for color, marker in zip(colors, markers)]\n",
    "    fig.legend(handles, models, fontsize=27, loc='upper center', ncol=8, bbox_to_anchor=(0.5, 1.03))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/Users/6u0/Desktop/Article Submissions/Paper4/images/'f'{method}_rankings.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "metrics = ['F1', 'ROC', 'MCC']\n",
    "\n",
    "# Plot rankings for each method\n",
    "for method in ['WF', 'BW', 'SW']:\n",
    "    plot_ranking(combined_df, method, models, metrics, colors, markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5a653",
   "metadata": {},
   "source": [
    "## Split Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b9b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trial for mcc\n",
    "\n",
    "file_path = ''\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rank the MCC scores within each k-fold for each model\n",
    "df['Rank'] = df.groupby(['K-fold', 'Model'])['mcc'].rank(method='min', ascending=False)\n",
    "\n",
    "# Aggregate the ranks to compare split performance\n",
    "rank_aggregation = df.groupby(['K-fold', 'split'])['Rank'].mean().reset_index()\n",
    "\n",
    "rank_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d4428",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mcc\n",
    "\n",
    "file_path_bw = '/mcc/SortedMCC_tscv_bw.csv'\n",
    "file_path_sw = '/mcc/SortedMCC_tscv_sw.csv'\n",
    "file_path_wf = '/mcc/SortedMCC_tscv_wf.csv'\n",
    "\n",
    "df_bw = pd.read_csv(file_path_bw)\n",
    "df_sw = pd.read_csv(file_path_sw)\n",
    "df_wf = pd.read_csv(file_path_wf)\n",
    "\n",
    "def rank_and_aggregate(df):\n",
    "    df['Rank'] = df.groupby(['K-fold', 'Model'])['mcc'].rank(method='min', ascending=False)\n",
    "    rank_aggregation = df.groupby(['K-fold', 'split'])['Rank'].mean().reset_index()\n",
    "    return rank_aggregation\n",
    "\n",
    "rank_aggregation_wf = rank_and_aggregate(df_wf)\n",
    "rank_aggregation_bw = rank_and_aggregate(df_bw)\n",
    "rank_aggregation_sw = rank_and_aggregate(df_sw)\n",
    "\n",
    "rank_aggregation_wf['Method'] = 'wf'\n",
    "rank_aggregation_bw['Method'] = 'bw'\n",
    "rank_aggregation_sw['Method'] = 'sw'\n",
    "\n",
    "combined_ranks = pd.concat([rank_aggregation_wf, rank_aggregation_bw, rank_aggregation_sw], ignore_index=True)\n",
    "\n",
    "combined_ranks\n",
    "output_file_path = '/split_rank_mcc.csv'\n",
    "combined_ranks.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1\n",
    "\n",
    "file_path_bw = '/f1/SortedF1_tscv_bw.csv'\n",
    "file_path_sw = '/f1/SortedF1_tscv_sw.csv'\n",
    "file_path_wf = '/f1/SortedF1_tscv_wf.csv'\n",
    "\n",
    "df_bw = pd.read_csv(file_path_bw)\n",
    "df_sw = pd.read_csv(file_path_sw)\n",
    "df_wf = pd.read_csv(file_path_wf)\n",
    "\n",
    "def rank_and_aggregate(df):\n",
    "    df['Rank'] = df.groupby(['K-fold', 'Model'])['f1_score'].rank(method='min', ascending=False)\n",
    "    rank_aggregation = df.groupby(['K-fold', 'split'])['Rank'].mean().reset_index()\n",
    "    return rank_aggregation\n",
    "\n",
    "# Apply the function to all three dataframes\n",
    "rank_aggregation_wf = rank_and_aggregate(df_wf)\n",
    "rank_aggregation_bw = rank_and_aggregate(df_bw)\n",
    "rank_aggregation_sw = rank_and_aggregate(df_sw)\n",
    "\n",
    "rank_aggregation_wf['Method'] = 'wf'\n",
    "rank_aggregation_bw['Method'] = 'bw'\n",
    "rank_aggregation_sw['Method'] = 'sw'\n",
    "\n",
    "combined_ranks = pd.concat([rank_aggregation_wf, rank_aggregation_bw, rank_aggregation_sw], ignore_index=True)\n",
    "\n",
    "combined_ranks\n",
    "output_file_path = '/Users/6u0/Desktop/Article Submissions/Paper4/final_results/split_rank_f1.csv'\n",
    "combined_ranks.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d910514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc\n",
    "\n",
    "file_path_bw = '/roc/SortedROC_tscv_bw.csv'\n",
    "file_path_sw = '/roc/SortedROC_tscv_sw.csv'\n",
    "file_path_wf = '/roc/SortedROC_tscv_wf.csv'\n",
    "\n",
    "df_bw = pd.read_csv(file_path_bw)\n",
    "df_sw = pd.read_csv(file_path_sw)\n",
    "df_wf = pd.read_csv(file_path_wf)\n",
    "\n",
    "def rank_and_aggregate(df):\n",
    "    df['Rank'] = df.groupby(['K-fold', 'Model'])['roc_auc_score'].rank(method='min', ascending=False)\n",
    "    rank_aggregation = df.groupby(['K-fold', 'split'])['Rank'].mean().reset_index()\n",
    "    return rank_aggregation\n",
    "\n",
    "rank_aggregation_wf = rank_and_aggregate(df_wf)\n",
    "rank_aggregation_bw = rank_and_aggregate(df_bw)\n",
    "rank_aggregation_sw = rank_and_aggregate(df_sw)\n",
    "\n",
    "rank_aggregation_wf['Method'] = 'wf'\n",
    "rank_aggregation_bw['Method'] = 'bw'\n",
    "rank_aggregation_sw['Method'] = 'sw'\n",
    "\n",
    "combined_ranks = pd.concat([rank_aggregation_wf, rank_aggregation_bw, rank_aggregation_sw], ignore_index=True)\n",
    "\n",
    "combined_ranks\n",
    "output_file_path = '/split_rank_roc.csv'\n",
    "combined_ranks.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e247283",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = ''\n",
    "\n",
    "mcc_file_path = path + 'split_rank_mcc.csv'\n",
    "roc_file_path = path + 'split_rank_roc.csv'\n",
    "f1_file_path = path + 'split_rank_f1.csv'\n",
    "\n",
    "df_mcc = pd.read_csv(mcc_file_path)\n",
    "df_roc = pd.read_csv(roc_file_path)\n",
    "df_f1 = pd.read_csv(f1_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0901876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='split', y='Rank', hue='Method', data=df_mcc)\n",
    "plt.title('MCC: Distribution of Ranks for Each Split Across Different Methods and K-folds')\n",
    "plt.xlabel('Split')\n",
    "plt.ylabel('Rank')\n",
    "plt.legend(title='Method')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(x='split', y='Rank', hue='Method', data=df_mcc, marker='o')\n",
    "plt.title('MCC: Average Rank per Split for Each Method')\n",
    "plt.xlabel('Split')\n",
    "plt.ylabel('Average Rank')\n",
    "plt.legend(title='Method')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='split', y='Rank', hue='Method', data=df_mcc)\n",
    "plt.title('MCC: Comparison of Average Ranks of Splits Across Methods and K-folds')\n",
    "plt.xlabel('Split')\n",
    "plt.ylabel('Average Rank')\n",
    "plt.legend(title='Method')\n",
    "plt.show()\n",
    "\n",
    "heatmap_data = df_mcc.pivot_table(index='K-fold', columns=['Method', 'split'], values='Rank', aggfunc='mean')\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='viridis', linewidths=0.5)\n",
    "plt.title('Heatmap of Split Rankings for MCC Across Methods and K-folds')\n",
    "plt.xlabel('Method and Split')\n",
    "plt.ylabel('K-fold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455df137",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# heatmaps with standard colorbar:\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_heatmap(df, title, vmin, vmax):\n",
    "    heatmap_data = df.pivot_table(index='K-fold', columns=['Method', 'split'], values='Rank', aggfunc='mean')\n",
    "    sns.heatmap(heatmap_data, fmt=\".2f\", annot=True, cmap='viridis', linewidths=0.5, annot_kws={\"size\": 17}, vmin=vmin, vmax=vmax)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.xlabel('Method and Split', fontsize=20)\n",
    "    plt.ylabel('K-fold', fontsize=20)\n",
    "\n",
    "combined_df = pd.concat([df_mcc, df_roc, df_f1])\n",
    "vmin = combined_df['Rank'].min()\n",
    "vmax = combined_df['Rank'].max()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "gs = plt.GridSpec(3, 1, hspace=5)\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "create_heatmap(df_mcc, 'Split Rankings for MCC Across Methods and K-folds', vmin, vmax)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "create_heatmap(df_roc, 'Split Rankings for ROC Across Methods and K-folds', vmin, vmax)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "create_heatmap(df_f1, 'Split Rankings for F1 Across Methods and K-folds', vmin, vmax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/6u0/Desktop/Article Submissions/Paper4/images/split_rankings.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_heatmap(df, title, vmin, vmax, filename):\n",
    "    plt.figure(figsize=(20, 6))  # Adjust the figsize as needed\n",
    "    heatmap_data = df.pivot_table(index='K-fold', columns=['Method', 'split'], values='Rank', aggfunc='mean')\n",
    "    sns.heatmap(heatmap_data, fmt=\".2f\", annot=True, cmap='viridis', linewidths=0.5, annot_kws={\"size\": 17}, vmin=vmin, vmax=vmax)\n",
    "    #plt.title(title, fontsize=30)\n",
    "    plt.xlabel('Method and Split', fontsize=20)\n",
    "    plt.ylabel('K-fold', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "combined_df = pd.concat([df_mcc, df_roc, df_f1])\n",
    "vmin = combined_df['Rank'].min()\n",
    "vmax = combined_df['Rank'].max()\n",
    "\n",
    "create_heatmap(df_mcc, 'Split Rankings for MCC Across Methods and K-folds', vmin, vmax, '/Users/6u0/Desktop/Article Submissions/Paper4/images/split_rankings_mcc.png')\n",
    "create_heatmap(df_roc, 'Split Rankings for ROC Across Methods and K-folds', vmin, vmax, '/Users/6u0/Desktop/Article Submissions/Paper4/images/split_rankings_roc.png')\n",
    "create_heatmap(df_f1, 'Split Rankings for F1 Across Methods and K-folds', vmin, vmax, '/Users/6u0/Desktop/Article Submissions/Paper4/images/split_rankings_f1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fcb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to create separate heatmaps for each TSCV method, sharing a colorbar\n",
    "def create_separate_heatmaps(df, metric_name, vmin, vmax):\n",
    "    methods = ['bw', 'sw', 'wf']\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "\n",
    "    for i, method in enumerate(methods):\n",
    "        method_data = df[df['Method'] == method].pivot(index='K-fold', columns='split', values='Rank')\n",
    "        sns.heatmap(method_data, fmt=\".2f\", linewidths=0.5, ax=axs[i], vmin=vmin, vmax=vmax, annot=True, annot_kws={\"size\": 17},\n",
    "                    cmap='viridis', cbar=i == 2)\n",
    "        #axs[i].set_title(f'{method.upper()} - {metric_name}', fontsize=15)\n",
    "        axs[i].set_xlabel('Split', fontsize=12)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel('K-fold', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#sns.heatmap(heatmap_data, fmt=\".2f\", annot=True, cmap='viridis', linewidths=0.5, annot_kws={\"size\": 17}, vmin=vmin, vmax=vmax)\n",
    "    \n",
    "vmin = combined_df['Rank'].min()  \n",
    "vmax = combined_df['Rank'].max()  \n",
    "\n",
    "create_separate_heatmaps(df_mcc, 'MCC', vmin, vmax)\n",
    "create_separate_heatmaps(df_roc, 'ROC', vmin, vmax)\n",
    "create_separate_heatmaps(df_f1, 'F1', vmin, vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea523aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to create separate heatmaps without titles\n",
    "def create_separate_heatmaps_fixed_box_size(df, vmin, vmax, box_height=1, box_width=1):\n",
    "    methods = ['bw', 'sw', 'wf']\n",
    "    nrows = df['K-fold'].nunique()  # Get the number of unique K-folds (rows)\n",
    "    ncols = df['split'].nunique()   # Get the number of splits (columns)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(ncols * box_width * 3, nrows * box_height), sharey=True)\n",
    "\n",
    "    for i, method in enumerate(methods):\n",
    "        method_data = df[df['Method'] == method].pivot(index='K-fold', columns='split', values='Rank')\n",
    "\n",
    "        sns.heatmap(method_data, ax=axs[i], vmin=vmin, vmax=vmax, annot=True, cmap='viridis',\n",
    "                    cbar=i == 2, cbar_kws={\"shrink\": 0.8} if i == 2 else None, \n",
    "                    linewidths=0.5, square=True)\n",
    "\n",
    "        axs[i].set_xlabel('Split', fontsize=12)\n",
    "\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel('K-fold', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example of using the function for three metrics (MCC, ROC, F1)\n",
    "vmin = combined_df['Rank'].min()  # Set vmin for consistent colorbar scaling\n",
    "vmax = combined_df['Rank'].max()  # Set vmax for consistent colorbar scaling\n",
    "\n",
    "# Generate heatmaps without titles\n",
    "create_separate_heatmaps_fixed_box_size(df_mcc, vmin, vmax)\n",
    "create_separate_heatmaps_fixed_box_size(df_roc, vmin, vmax)\n",
    "create_separate_heatmaps_fixed_box_size(df_f1, vmin, vmax)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
